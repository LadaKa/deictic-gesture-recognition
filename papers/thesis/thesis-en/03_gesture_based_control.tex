\chapter{Gesture Based Robot Control}

\section{Gesture Based "Pick And Place"}
Two gestures are required to successfully perform the "Pick And Place" task:
a deictic gesture to indicate the position and another gesture for confirmation.\par
During the selection of the object and the target location, the user should face the camera, with all objects lying on the floor between him and the camera. The scene is displayed on the rViz screen. Once the objects are detected by the vision system, their images are marked in blue.\par
The user can then initiate the selection of an object by pointing at it with his right hand. To confirm the gesture, he raises his left hand while still pointing at the object.\par
The object closest to the intersection of the pointing ray and the floor is selected. Its image is marked in red.\par
We can then specify the target location of the selected object. The user determines the location in the same way as before: by pointing to the location and raising his hand.\par
The target location is on the floor and  has to be selected inside the safety frame that is shown in the rViz. The frame represents a space that is safe for the robot to move around, there are no obstacles except for the detected objects.\par
Once the target location is selected, it is marked in red in rViz, gesture detection is completed and the resulting data is sent to the robot.\par

\section{Gestures}

\subsection{Pointing Gesture}
The user can choose from three types of pointing gestures. Each type is represented by a pair of joints that corresponds to the pointing ray. The first joint in the pair is the origin and the second determines the direction of the ray:\par
\begin{itemize}
	\item Shoulder, wrist (default option).
    \item Elbow, wrist.
    \item Head, hand.
\end{itemize}

The pointing gesture indicates the point where the pointing ray intersects the floor and allows us to select an object or its target location.\par
The pointing ray is displayed from its origin to the intersection with the floor.\par
The pointing gesture has to be performed with the right hand and the first joint has to be positioned higher than the second. These constraints help to reduce the number of falsely detected gestures.\par

\subsection{Raising Hand Gesture}
The hand gesture consists of lifting the hand. It has to be performed with the left arm and the hand  has to be raised above the head.\par
When pointing with the right arm, the user confirms the pointing gesture by raising the left hand. If no pointing gesture is performed in the moment, the raising hand gesture is ignored.\par

\section{Gesture Recognition with ORBBEC Astra SDK}

\subsection{Limitations}
ORBBEC Astra SDK provides tools for skeleton recognition and person tracking. The maximum distance for skeleton recognition is 4 meters. Multiple persons can be tracked at the same time.\par
The skeleton is represented by a set of joints and their positions. The head corresponds to single joint, eye positions and other details are unavailable.\par
Three joints are given for each arm: shoulder, elbow and hand. The fingers are not recognizable.\par
SDK also supplies the detection of the grip gesture. I considered using it as a confirmation gesture but preferred the hand raising gesture because the grip was often not detected.\par

\subsection{Occlusion}
The most common cause of gesture recognition errors is occlusion. Only relatively small objects (not taller than 20 cm) were used for the experiments. If the objects were larger, many false detections occurred because the objects often obscured parts of the person's body and the joints of the corresponding skeleton were not correctly identified.\par
Occlusion also often occurs when more than one person is in front of the camera. Therefore, it is better to perform the Pick And Place task when only one person is in the scene because it makes gesture recognition more robust.\par
However, if more than one person is present, each person can perform a pointing gesture to select an object or a target location.\par

\subsection{Recommendations}
When I tested skeleton recognition with a single person and small objects, most of the errors were caused by the person's posture.\par
For best results, the person should be facing the camera, not turning the body or crossing his limbs, as this can lead to errors such as interrupted tracking of the person, misidentification of joints and false recognition of gestures.\par
In case of difficulties with skeleton recognition, it may help if the person moves closer to the camera or extends his arms out to the sides.\par