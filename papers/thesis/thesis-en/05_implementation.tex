\chapter{Implementation}

\section{Vision system}

\subsection{Image Processing with ORBBEC Astra Camera}
The image data is sent from the camera to the connected notebook for processing.\par
There is the ROS Master and several other individual ROS nodes running on the notebook. Some ROS nodes are involved in image processing, while others provide tools such as geometric calculations or displaying detected objects and skeletons on the rViz screen.\par
ROS nodes communicate with each other using ROS messages and share information about the progress of their subtasks.\par
I used two main tools to process the camera data: the ROS Astra camera driver "ros\_astra\_camera" for object detection and the "ORBBEC Astra SDK" for skeleton detection.\par
Both tools use OpenNI as an intermediate layer to access the camera data. I couldn't run them at the same time because it led to runtime errors, so I decided to split the process into two separate phases.\par
First, the "ros\_astra\_camera" driver is started. Once all objects are detected, the driver stops.\par
In the second phase, data is exposed using the "ORBBEC Astra SDK" until both pointing gestures are confirmed and the result is sent to the robot. \par
Since gesture recognition is performed within the ROS system, additional ROS packages were needed to publish the body tracking data provided by the SDK as ROS messages.\par

\subsection{Installation}

\subsubsection{ROS Noetic}
The notebook with Ubuntu 20.04 was used, for which the recommended version of the ROS distribution is ROS Noetic. I followed the instructions from http://wiki.ros.org/noetic/Installation/Ubuntu to download and install the ROS Noetic package.\par

\subsubsection{Astra and OpenNI SDKs}
For the ORBBEC Astra camera, I needed to install the Astra SDK and the OpenNI SDK for Linux.\par
Both SDKs are available at https://www.orbbec.com/developers.\par

\subsubsection{ROS Driver for Astra camera}
I downloaded the ROS driver package\\
from https://github.com/orbbec/ros\_astra\_camera and installed the dependencies according to the instructions on http://wiki.ros.org/astra\_camera.\par
The ros\_astra\_camera package supports the ROS distributions Kinetic and Melodic. I needed to find and test multiple versions of the "ros-*-libuvc-*" libraries, as they were not released specifically for ROS Noetic.\par
This problem was already solved on the ORBBEC GitHub page, so I followed the advice and installed the missing dependencies using:\par

\begin{lstlisting}[language=bash]
  $ apt install ros-noetic-rgbd-launch libuvc-dev
\end{lstlisting}


I built the package with "catkin\_make" command and was able to run code samples that show the camera data on the screen.\par

\subsubsection{ROS Packages for Gesture Based Control}
I downloaded three ROS packages from the Shinsel Robots repository on https://github.com/shinselrobots.\par
The "pcl\_object\_detection" package allows object detection in the camera data provided by the ROS Astra driver using the Point Cloud Library.\par
The "astra\_body\_tracker" and "body\_tracker\_msgs" packages publish body tracking data from the Astra SDK as ROS messages.\par
Several environment variables have to be set to indicate the paths to the AstraSDK and OpenNI subfolders.\par
For example, if "/home/user/AstraSDK" is the folder containing the Astra SDK and "/home/user/OpenNI-Linux-x64-2.3.0.66" is the folder containing the OpenNI SDK, the settings can be made by running these commands in the terminal: \par

\begin{lstlisting}[language=bash]
  $ export ASTRA_SDK=/home/user/AstraSDK
  $ export ASTRA_ROOT=/home/user/AstraSDK
  $ export ASTRA_SDK_INCLUDE=/home/user/AstraSDK/include
  $ export ASTRA_SDK_LIB=/home/user/AstraSDK/lib
  $ export OPENNI2_INCLUDE=/home/user/OpenNI-Linux-x64-2.3.0.66
  $ export OPENNI2_REDIST=/home/user/OpenNI-Linux-x64-2.3.0.66/Redist
\end{lstlisting}

\subsection{Source Code}

\subsubsection{Catkin Workspace} 
Catkin is the official build system for ROS. Project packages that are placed in the same catkin workspace can be built all at once.\par
My catkin workspace folder contains following ROS packages, all with source code written in C++: \par

\begin{itemize}
	\item ros\_astra\_camera
    \item task\_execution
    \item rviz\_screen
    \item pcl\_object\_detection
    \item pointing\_gesture
\end{itemize}

\subsubsection{Program Overview} 
  
The main launch file is task\_execution.launch. It starts the ROS Astra driver, rViz and other ROS nodes involved in the task: task\_execution\_node, pcl\_object\_detection\_node and pointing\_gesture\_node.\par
The task\_execution\_node subscribes to ROS messages\\
"pcl\_object\_detection/detected\_objects" and "body\_tracker/intersection".\\
The "pcl\_object\_detection/detected\_objects" message contains an array of coordinates of the detected objects, the "body\_tracker/intersection" message contains the coordinates of the intersection of the pointing ray and the floor.\par
When the intersection message is received for the first time, the nearest object is calculated. The object is represented by its index in the detected object array, which is then published in the "task\_execution/pointed\_object\_index" message.\par
The second received intersection message indicates the target location.
Once received, the node creates a result file and writes the coordinates of all detected objects, the coordinates of the target location and the index of the selected object.\par
Then the node connects to the robot's computer using SSH, transfers the file there and remotely starts the robot's main program.\par

\subsubsection{Object Detection with Point Cloud Library}
Point Cloud Library (PCL) was used as a tool for processing the image data.\par
The pcl\_object\_detection package allows the detection of objects on a flat surface and depends on two PCL packages: pcl\_ros and pcl\_conversions.\par
The pcl\_ros package bridges ROS systems and 3D applications that work with point clouds. It extends the ROS C++ Client Library to support messages with native PCL data types.\par
Conversions between PCL data types and ROS message types are provided by the pcl\_conversions package.\par
The pcl\_object\_detection\_node subscribes to the topic \\"/astra\_camera/depth/points", which is published by the ROS Astra camera driver. The received messages represent a point cloud with no color information.\par
The point cloud is processed using PCL: the data is filtered with VoxelGrid and used for plane segmentation. The remaining points, which are outside the plane, are divided into clusters.
Each cluster can eventually be considered as a detected object. We can specify parameters that limit the height and width of the detected objects.\par
All clusters that meet the given conditions correspond to detected objects and their properties are published in the "pcl\_object\_detection/detected\_objects" message.\par

\subsubsection{Detection in Experimental Environment}
Even if the scene with the objects was static, different objects were detected in consecutive depth clouds. The detection reliability decreased with the person moving in the scene.\par
I modified the original pcl\_object\_detection package from Shinsel Robots to improve the quality of object detection in my experimental environment. I also needed to implement a mechanism for switching between the ROS Astra driver and the Astra SDK to avoid issues with camera data accessibility.\par
The object size is limited to 20 cm and the object can be detected only when lying on the floor. Furthermore, a detection frame has been specified as a constraint for the object's location on the floor. An object outside the detection frame is ignored.\par
To ensure that the same objects are detected when the experiment is repeated in the same scene, the total number of objects is fixed and the program is modified:\par
The received point clouds are processed one by one, with some objects detected in each cloud. \par
If the number of detected objects does not equal the total number, the result is ignored and processing continues with the next cloud.\par
Otherwise, the number of detected objects is correct, which usually means that the correct objects have been detected. Their data is published in the "pcl\_object\_detection/detected\_objects" message. In addition, the message "object\_detection\_done" is published, which indicates that the ROS Astra driver is no longer needed.\par 
I added a subscriber to the ROS Astra driver to the "object\_detection\_done" message. Once the message is received, the driver is shut down.\par

The package was also extended by code for displaying detected objects in rViz, marking them in blue and changing the color to red if the object was selected by the  pointing gesture.\par

\subsubsection{Gesture recognition}
The pointing\_gesture package implements methods for gesture recognition. It uses the Astra SDK to obtain body tracking information and allows several types of gesture recognition.\par
I modified the code from the astra\_body\_tracker package from Shinsels Robots. The package publishes body tracking information from the Astra SDK in the ROS topic, using messages from the body\_tracker\_msgs package.\par
The Astra SDK provides tools for skeleton recognition and body tracking. The tracked person is represented by a list of nineteen joints.\par
Corresponding body data includes the position and status of each joint, where status indicates the confidence level of the tracking with possible values of "NotTracked", "LowConfidence" and "Tracked".
The data also contains the current body tracking status, body orientation and hand pose.\par
The pointing\_gesture node is inactive during object detection. 
The "object\_detection\_done" message triggers the shutting down of the ROS Astra driver.\par
I added a subscriber for the "object\_detection\_done" topic to the pointing gesture node as well. Once the message is received, the node attempts to open the Astra data stream using the Astra SDK and repeats this until the ROS Astra device stream is completely terminated.\par
Then the data stream is available to the SDK and we can start gesture recognition.\par 
The SDK provides a list of tracked persons for each frame of the image data stream. For each tracked person, a check is made to see if he performs the confirmation and pointing gestures.\par
The confirmation gesture is performed by raising the left hand. It is detected when the left hand joint is raised above the head joint.\par
I tried using the shoulder joint instead of the head joint. However, body tracking is not completely reliable and I observed many cases of false gesture detection. The shoulder joint has been often misidentified, while the head joint is usually identified correctly.\par
If a confirmation gesture is detected in a given frame, the right arm is checked for the pointing gesture, otherwise processing continues with the next frame.\par
The pointing gesture is recognized by the position of the pair of joints corresponding to the selected pointing gesture type. The first joint of the pair determines the origin of the pointing ray. It is the upper joint considering the standard anatomical position.\par
The pointing gesture is detected when the first joint is positioned higher than the second.\par
The coordinates of the pointing ray intersection with the floor plane are computed and published in a ROS message.\par
The default option for the pointing gesture is the head-wrist type. The user may select a different option using a ROS message (TODO: command, head).\par

I have added rViz markers to display the skeleton of the person being tracked and the pointing ray. When the first confirmed pointing gesture is detected and the intersection coordinates are published, the object closest to the intersection is selected and marked in red in rViz. The distance between the intersection point and the object is logged.\par
The second detected pointing gesture specifies the target location. The coordinates of the corresponding intersection are also published and marked in rViz.\par
Once the second confirmed pointing gesture is detected, the data stream is closed.\par


\subsubsection{TODO - some notes}
Why I choose ORBBEC Astra camera over Kinect ONE (v2):\\
difficult installation of tools and libraries for a ROS Interface to the Kinect One (dependencies on ROS Hydro/Indigo distribution, no available packages for ROS Noetic).\\
New ORBBEC Astra ROS SDK.\\

\section{Navigation of Autonomous Vehicle}
TODO:\\
\subsection{Installation}
Neobotix:

Packages:
https://github.com/neobotix/

ros-noetic-amcl, ros-noetic-map-server, ros-noetic-move-base, ...

\subsection{Map of Environment}
Mapping procedure, selecting the map for navigation, visualization with RViz...\\

\subsection{Navigation to Goal}
Goal definition, movement (path, obstacle avoidance, ...).

\section{Object Manipulation}
TODO:\\

\subsection{Installation}
Universal Robots:\\

Packages:\\
Universal\_Robots\_ROS\_Driver
https://github.com/UniversalRobots/Universal\_Robots\_ROS\_Driver\\

Universal\_Robots\_Client\_Library\\
https://github.com/UniversalRobots/Universal\_Robots\_Client\_Library\\

ur5\_moveit\_config\\
https://github.com/ros-industrial/universal\_robot/tree/noetic-devel/ur5\_moveit\_config\\



\subsection{Mobile Manipulator URDF}
URDF for Neobotix, UR5 and gripper.\\

\subsection{MoveIt Setup Assistant}
How to create config and set up arm positions.\\
How to set up arm limits.\\
Simulation in rViz.\\

\subsection{Code}
ur\_robot\_driver;\\
ROS.urp;\\
move\_it\_planning;\\
trajectory commands;\\

\subsection{Objects coordinates}
approximate coordinates of objects obtained from the vision system; \\
robot navigates to objects; \\
exact objects coordinates from LIDAR (lidar\_scan topic subscriber).\\










