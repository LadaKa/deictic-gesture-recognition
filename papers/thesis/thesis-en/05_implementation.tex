\chapter{Implementation}

\section{Vision system}

\subsection{Image Processing with ORBBEC Astra Camera}
The image data is sent from the camera to the connected laptop for processing.\par
There is the ROS Master and several other individual ROS nodes running on the notebook. Some ROS nodes are involved in image processing, while others provide tools such as geometric calculations or displaying detected objects and skeletons on the rViz screen.\par
ROS nodes communicate with each other using ROS messages and share information about the progress of their subtasks.\par
I used two main tools to process the camera data: the ROS Astra camera driver "ros\_astra\_camera" for object detection and the "ORBBEC Astra SDK" for skeleton detection.\par
Both tools use OpenNI as an intermediate layer to access the camera data. I couldn't run them at the same time because it led to runtime errors, so I decided to split the process into two separate phases.
First, the "ros\_astra\_camera" driver is started. Once all objects are detected, the driver stops.\par
In the second phase, data is exposed using the "ORBBEC Astra SDK" until both pointing gestures are confirmed and the result is sent to the robot. Since gesture recognition is performed within the ROS system, additional ROS packages were needed to publish the body tracking data provided by the SDK as ROS messages.\par

\subsection{Installation}

\subsubsection{ROS Noetic}
The notebook with Ubuntu 20.04 was used, for which the recommended version of the ROS distribution is ROS Noetic. I followed the instructions from http://wiki.ros.org/noetic/Installation/Ubuntu to download and install the ROS Noetic package.\par

\subsubsection{Astra and OpenNI SDKs}
For the ORBBEC Astra camera, I needed to install the Astra SDK and the OpenNI SDK for Linux.\par
Both SDKs are available at https://www.orbbec.com/developers.\par

\subsubsection{ROS Driver for Astra camera}
I downloaded the ROS driver package\\
from https://github.com/orbbec/ros\_astra\_camera and installed the dependencies according to the instructions on http://wiki.ros.org/astra\_camera.\par
The "ros\_astra\_camera" package supports the ROS distributions Kinetic and Melodic. I needed to find and test multiple versions of the "ros-*-libuvc-*" libraries, as they were not released specifically for ROS Noetic.\par
This problem was already solved on the ORBBEC GitHub page, so I followed the advice and installed the missing dependencies using:\par

\begin{lstlisting}[language=bash]
  $ apt install ros-noetic-rgbd-launch libuvc-dev
\end{lstlisting}


I built the package with "catkin\_make" command and was able to run code samples that show the camera data on the screen.\par

\subsubsection{ROS Packages for Gesture Based Control}
I downloaded three ROS packages from the Shinsel Robots repository on https://github.com/shinselrobots.\par
The "pcl\_object\_detection" package allows object detection in the camera data provided by the ROS Astra driver using the Point Cloud Library.\par
The "astra\_body\_tracker" and "body\_tracker\_msgs" packages publish body tracking data from the Astra SDK as ROS messages.\par
Several environment variables have to be set to indicate the paths to the AstraSDK and OpenNI subfolders.\par
For example, if "/home/user/AstraSDK" is the folder containing the Astra SDK and "/home/user/OpenNI-Linux-x64-2.3.0.66" is the folder containing the OpenNI SDK, the settings can be made by running these commands in the terminal: \par

\begin{lstlisting}[language=bash]
  $ export ASTRA_SDK=/home/user/AstraSDK
  $ export ASTRA_ROOT=/home/user/AstraSDK
  $ export ASTRA_SDK_INCLUDE=/home/user/AstraSDK/include
  $ export ASTRA_SDK_LIB=/home/user/AstraSDK/lib
  $ export OPENNI2_INCLUDE=/home/user/OpenNI-Linux-x64-2.3.0.66
  $ export OPENNI2_REDIST=/home/user/OpenNI-Linux-x64-2.3.0.66/Redist
\end{lstlisting}

\subsection{The Vision System Code}

\subsubsection{Catkin Workspace} 
Catkin is the official build system for ROS. Project packages that are placed in the same catkin workspace can be built all at once.\par
My catkin workspace folder contains following ROS packages: \par

\begin{itemize}
	\item ros\_astra\_camera
    \item task\_execution
    \item rviz\_screen
    \item pcl\_object\_detection
    \item pointing\_gesture
\end{itemize}

\subsubsection{Program Overview} 
  
The main launch file is task\_execution.launch. It starts the ROS Astra driver, rViz and other ROS nodes involved in the task: task\_execution\_node, pcl\_object\_detection\_node and pointing\_gesture\_node.\par
The task\_execution\_node subscribes to ROS messages\\
"pcl\_object\_detection/detected\_objects" and "body\_tracker/intersection".\\
The "pcl\_object\_detection/detected\_objects" message contains an array of coordinates of the detected objects, the "body\_tracker/intersection" message contains the coordinates of the intersection of the pointing ray and the floor.\par
When the intersection message is received for the first time, the nearest object is calculated. The object is represented by its index in the detected object array, which is then published in the "task\_execution/pointed\_object\_index" message.\par
The second received intersection message indicates the target location.
Once received, the node creates a result file and writes the coordinates of all detected objects, the coordinates of the target location and the index of the selected object.\par
Then the node connects to the robot's computer using SSH, transfers the file there and remotely starts the robot's main program.\par

\subsubsection{Object Detection with Point Cloud Library}
Point Cloud Library (PCL) was used as a tool for processing the image data.\par
The pcl\_object\_detection package allows the detection of objects on a flat surface and depends on two PCL packages: pcl\_ros and pcl\_conversions.\par
The pcl\_ros package bridges ROS systems and 3D applications that work with point clouds. It extends the ROS C++ Client Library to support messages with native PCL data types.\par
Conversions between PCL data types and ROS message types are provided by the pcl\_conversions package.\par
The pcl\_object\_detection\_node subscribes to the topic \\"/astra\_camera/depth/points", which is published by the ROS Astra camera driver. The received messages represent a point cloud with no color information.\par
The point cloud is processed using PCL: the data is filtered with VoxelGrid and used for plane segmentation. The remaining data, which is out-of-plane, is divided into clusters.
Each cluster can eventually be considered as a detected object. We can specify parameters that limit the height and width of the detected objects.\par
All clusters that meet the given conditions correspond to detected objects and their properties are published in the "pcl\_object\_detection/detected\_objects" message.\par

\subsubsection{Code Adaptation to Experimental Environment}
Even if the scene with the objects was static, different objects were detected in consecutive depth clouds. The detection reliability decreased with the person moving in the scene.\par
I modified the original pcl\_object\_detection package from Shinsel Robots to improve the quality of object detection in my experimental environment. I also needed to implement a mechanism for switching between the ROS Astra driver and the Astra SDK to avoid problems with camera data accessibility.\par
The object size is limited to 20 cm and the object can be detected only when lying on the floor. Furthermore, a detection frame has been specified as a constraint for the object's location on the floor. An object outside the detection frame is ignored.\par
To ensure that the same objects are detected when the experiment is repeated in the same scene, the total number of objects is fixed and the program is modified:\par
The received point clouds are processed one by one, with some objects detected in each cloud. \par
If the number of detected objects does not equal the total number, the result is ignored and processing continues with the next cloud.\par
Otherwise, the number of detected objects is correct, which usually means that the correct objects have been detected. Their data is published in the "pcl\_object\_detection/detected\_objects" message. In addition, the message "object\_detection\_done" is published, which indicates that the ROS Astra driver is no longer needed.\par 
I added a subscriber to the ROS Astra driver to the "object\_detection\_done" message. Once the message is received, the driver is shut down.\par
The package has also been extended with code to display detected objects in rViz, mark them with a color, and change the color of the object that was selected by the pointing gesture.\par

\subsection{Gesture detection}
Packages: 
astra\_body\_tracker:\\
https://github.com/shinselrobots/astra\_body\_tracker\\
Publisher of ROS topic for body tracking information (from the ORBBEC SDK).\\

pointing\_gesture:\\
modified astra\_body\_tracker package to get skeleton data;\\
added code to detect gestures, rViz markers, ....\\

Publisher of pointing\_gesture topic (as geometry\_msgs).\\


\subsection{Object selection and target location}
task\_control\_node (will be renamed):\\
Subscriber to object detection and pointing gesture topics;\\
provides calculations of pointing ray intersection and selection of object.\\
Sends data to mobile manipulator PC over SSH (coordinates of objects and target location, info about selected object).\\

\subsubsection{TODO - some notes}
Why I choose ORBBEC Astra camera over Kinect ONE (v2):\\
difficult installation of tools and libraries for a ROS Interface to the Kinect One (dependencies on ROS Hydro/Indigo distribution, no available packages for ROS Noetic).\\
new wrapper!!!!!!!!!
Installation of ORBBEC SDK for Linux and dependencies (OpenNI2, libsfml-dev, ...).\\

 Package ros\_astra\_camera:\\
 https://github.com/orbbec/ros\_astra\_camera\\
 OpenNI2 ROS wrapper for Orbbec 3D cameras.\\



\section{Navigation of Autonomous Vehicle}

\subsection{Installation}
Neobotix:

Packages:
https://github.com/neobotix/

ros-noetic-amcl, ros-noetic-map-server, ros-noetic-move-base, ...

\subsection{Map of Environment}
Mapping procedure, selecting the map for navigation, visualization with RViz...\\

\subsection{Navigation to Goal}
Goal definition, movement (path, obstacle avoidance, ...).

\section{Object Manipulation}


\subsection{Installation}
Universal Robots:\\

Packages:\\
Universal\_Robots\_ROS\_Driver
https://github.com/UniversalRobots/Universal\_Robots\_ROS\_Driver\\

Universal\_Robots\_Client\_Library\\
https://github.com/UniversalRobots/Universal\_Robots\_Client\_Library\\

ur5\_moveit\_config\\
https://github.com/ros-industrial/universal\_robot/tree/noetic-devel/ur5\_moveit\_config\\



\subsection{Mobile Manipulator URDF}
URDF for Neobotix, UR5 and gripper.\\

\subsection{MoveIt Setup Assistant}
How to create config and set up arm positions.\\
How to set up arm limits.\\
Simulation in rViz.\\

\subsection{Code}
ur\_robot\_driver;\\
ROS.urp;\\
move\_it\_planning;\\
trajectory commands;\\

\subsection{Objects coordinates}
approximate coordinates of objects obtained from the vision system; \\
robot navigates to objects; \\
exact objects coordinates from LIDAR (lidar\_scan topic subscriber).\\










